{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 : Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing, decomposition, discriminant_analysis, tree\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_credit_application = pd.read_csv('data/application_train.csv')\n",
    "raw_test_credit_application = pd.read_csv('data/application_test.csv')\n",
    "\n",
    "raw_bureau = pd.read_csv('data/bureau.csv')\n",
    "raw_bureau_balance = pd.read_csv('data/bureau_balance.csv')\n",
    "raw_credit_card_balance = pd.read_csv('data/credit_card_balance.csv')\n",
    "raw_installments_payments = pd.read_csv('data/installments_payments.csv')\n",
    "raw_pos_cash_balance = pd.read_csv('data/POS_CASH_balance.csv')\n",
    "raw_previous_application = pd.read_csv('data/previous_application.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2 : Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(raw_data, verbose=False):\n",
    "    raw_bureau_credit_status_vc = raw_bureau.groupby('SK_ID_CURR')['CREDIT_ACTIVE'].value_counts()\n",
    "    raw_bureau_credit_type_vc = raw_bureau.groupby('SK_ID_CURR')['CREDIT_TYPE'].value_counts()\n",
    "    \n",
    "    raw_bureau_balance_without_C = raw_bureau_balance[raw_bureau_balance['STATUS'] != 'C']\n",
    "    raw_bureau_balance_new = pd.merge(left=raw_bureau_balance_without_C, right=raw_bureau[['SK_ID_BUREAU', 'SK_ID_CURR']], on='SK_ID_BUREAU')\n",
    "    raw_bureau_balance_vc = raw_bureau_balance_new.groupby(['SK_ID_CURR']).STATUS.value_counts()\n",
    "    \n",
    "    raw_previous_application_vc = raw_previous_application['SK_ID_CURR'].value_counts()\n",
    "    raw_previous_application_mean_amt_credit = raw_previous_application.groupby(['SK_ID_CURR'])['AMT_CREDIT'].mean()\n",
    "    raw_previous_application_name_contract_status_vc = raw_previous_application.groupby(['SK_ID_CURR'])['NAME_CONTRACT_STATUS'].value_counts(normalize=True)\n",
    "    raw_previous_application_mean_cnt_payment = raw_previous_application.groupby(['SK_ID_CURR'])['CNT_PAYMENT'].mean()\n",
    "    \n",
    "    additional_data = []\n",
    "    \n",
    "    SK_ID_CURR_list = raw_data['SK_ID_CURR'].unique()\n",
    "    SK_ID_CURR_list_in_raw_bureau = raw_bureau['SK_ID_CURR'].unique()\n",
    "    SK_ID_CURR_list_in_raw_previous_application = raw_previous_application['SK_ID_CURR'].unique()\n",
    "    \n",
    "    for index, SK_ID_CURR in enumerate(SK_ID_CURR_list):\n",
    "        if verbose and index % 10000 == 0:\n",
    "            print(index, time.time())\n",
    "            \n",
    "        # BUREAU\n",
    "        if SK_ID_CURR in SK_ID_CURR_list_in_raw_bureau:\n",
    "            bureau_SK_ID_CURR = raw_bureau.loc[raw_bureau['SK_ID_CURR'] == SK_ID_CURR]\n",
    "            \n",
    "            BUR_found = 1\n",
    "\n",
    "            temp = raw_bureau_credit_status_vc[SK_ID_CURR]\n",
    "            BUR_credit_active = temp['Active'] if 'Active' in temp else 0\n",
    "            BUR_credit_closed = temp['Closed'] if 'Closed' in temp else 0\n",
    "            BUR_credit_sold = temp['Sold'] if 'Sold' in temp else 0\n",
    "            BUR_credit_bad_debt = temp['Bad Debt'] if 'Bad Debt' in temp else 0\n",
    "\n",
    "            BUR_sum_credit_day_overdue = bureau_SK_ID_CURR['CREDIT_DAY_OVERDUE'].sum()\n",
    "            BUR_mean_credit_day_overdue = bureau_SK_ID_CURR['CREDIT_DAY_OVERDUE'].mean()\n",
    "\n",
    "            BUR_credit_type_count = raw_bureau_credit_type_vc.count()\n",
    "\n",
    "            BUR_mean_days_credit_update = bureau_SK_ID_CURR['DAYS_CREDIT_UPDATE'].mean()\n",
    "\n",
    "            # BUREAU_BALANCE\n",
    "            if SK_ID_CURR in raw_bureau_balance_vc:\n",
    "                BUR_BALANCE_found = 1\n",
    "                temp = raw_bureau_balance_vc[SK_ID_CURR]\n",
    "                BUR_bad_balance_score = (temp['X'] * 0.11 if 'X' in temp else 0 +  \\\n",
    "                            temp[1] if 1 in temp else 0 +  \\\n",
    "                            temp[2] * 2 if 2 in temp else 0 +  \\\n",
    "                            temp[3] * 3 if 3 in temp else 0 +  \\\n",
    "                            temp[4] * 4 if 4 in temp else 0 +  \\\n",
    "                            temp[5] * 5 if 5 in temp else 0) / temp.sum()\n",
    "            else:\n",
    "                BUR_BALANCE_found = 0\n",
    "                BUR_bad_balance_score = None\n",
    "        \n",
    "        else:\n",
    "            BUR_found = 0\n",
    "            BUR_credit_active = BUR_credit_closed = BUR_credit_sold = BUR_credit_bad_debt = 0\n",
    "            BUR_sum_credit_day_overdue = BUR_mean_credit_day_overdue = 0\n",
    "            BUR_credit_type_count = 0\n",
    "            BUR_mean_days_credit_update = 0\n",
    "            BUR_BALANCE_found = 0 \n",
    "            BUR_bad_balance_score = None\n",
    "            \n",
    "        # PREVIOUS APPLICATION\n",
    "        if SK_ID_CURR in SK_ID_CURR_list_in_raw_previous_application:\n",
    "            PREV_found = 1        \n",
    "            PREV_application_count = raw_previous_application_vc[SK_ID_CURR]     \n",
    "            PREV_mean_amount_credit = raw_previous_application_mean_amt_credit[SK_ID_CURR]\n",
    "            PREV_name_contract_status_approved = raw_previous_application_name_contract_status_vc[SK_ID_CURR]\n",
    "            PREV_mean_cnt_payment = raw_previous_application_mean_cnt_payment[SK_ID_CURR]\n",
    "        else:\n",
    "            PREV_found = 0\n",
    "            PREV_application_count = 0\n",
    "            PREV_mean_amount_credit = 0\n",
    "            PREV_name_contract_status_approved = 0\n",
    "            PREV_mean_cnt_payment = 0\n",
    "    \n",
    "        # Add a new row\n",
    "        additional_data.append([\n",
    "            SK_ID_CURR, \n",
    "                         \n",
    "            BUR_found,\n",
    "            BUR_credit_active,\n",
    "            BUR_credit_closed,\n",
    "            BUR_credit_sold,\n",
    "            BUR_credit_bad_debt,\n",
    "            BUR_sum_credit_day_overdue,\n",
    "            BUR_mean_credit_day_overdue,\n",
    "            BUR_credit_type_count,\n",
    "            BUR_mean_days_credit_update,\n",
    "            BUR_BALANCE_found,\n",
    "            BUR_bad_balance_score,\n",
    "            \n",
    "            PREV_application_count,\n",
    "            PREV_mean_amount_credit,\n",
    "            PREV_name_contract_status_approved,\n",
    "            PREV_mean_cnt_payment,\n",
    "        ])\n",
    "    \n",
    "    add_columns = [\n",
    "        'SK_ID_CURR',\n",
    "\n",
    "        'BUR_found',\n",
    "        'BUR_credit_active',\n",
    "        'BUR_credit_closed',\n",
    "        'BUR_credit_sold',\n",
    "        'BUR_credit_bad_debt',\n",
    "        'BUR_sum_credit_day_overdue',\n",
    "        'BUR_mean_credit_day_overdue',\n",
    "        'BUR_credit_type_count',\n",
    "        'BUR_mean_days_credit_update',\n",
    "        'BUR_BALANCE_found',\n",
    "        'BUR_bad_balance_score',\n",
    "\n",
    "        'PREV_application_count',\n",
    "        'PREV_mean_amount_credit',\n",
    "        'PREV_name_contract_status_approved',\n",
    "        'PREV_mean_cnt_payment',\n",
    "    ]\n",
    "    additional_data = pd.DataFrame(additional_data, columns=add_columns)\n",
    "    \n",
    "    return pd.merge(left=raw_data, right=additional_data, left_on='SK_ID_CURR', right_on='SK_ID_CURR', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1529179889.5494986\n",
      "10000 1529179944.6470284\n",
      "20000 1529179998.7372606\n",
      "30000 1529180048.466602\n",
      "40000 1529180098.3551698\n",
      "50000 1529180150.163805\n",
      "60000 1529180208.7301202\n",
      "70000 1529180263.0013087\n",
      "80000 1529180317.8578544\n",
      "90000 1529180372.8993251\n",
      "100000 1529180428.444007\n",
      "110000 1529180483.3204193\n",
      "120000 1529180537.843294\n",
      "130000 1529180590.3761835\n",
      "140000 1529180644.656096\n",
      "150000 1529180700.129253\n",
      "160000 1529180756.1597943\n",
      "170000 1529180811.2740352\n",
      "180000 1529180866.2490368\n",
      "190000 1529180919.568362\n",
      "200000 1529180968.0650737\n",
      "210000 1529181016.8065932\n",
      "220000 1529181066.371267\n",
      "230000 1529181115.840244\n",
      "240000 1529181166.6233191\n",
      "250000 1529181217.8735585\n",
      "260000 1529181268.291175\n",
      "270000 1529181319.475517\n",
      "280000 1529181368.288519\n",
      "290000 1529181417.74557\n",
      "300000 1529181467.1584024\n",
      "0 1529181524.684574\n",
      "10000 1529181576.7863204\n",
      "20000 1529181627.5013921\n",
      "30000 1529181680.2366307\n",
      "40000 1529181739.5657127\n"
     ]
    }
   ],
   "source": [
    "collected_train_credit_application = collect_data(raw_train_credit_application, verbose=True)\n",
    "collected_train_credit_application.to_pickle('collected_train_credit_application')\n",
    "\n",
    "collected_test_credit_application = collect_data(raw_test_credit_application, verbose=True)\n",
    "collected_test_credit_application.to_pickle('collected_test_credit_application')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_credit_application = raw_test_credit_application = raw_bureau = raw_bureau_balance = raw_credit_card_balance = raw_installments_payments = raw_pos_cash_balance = raw_previous_application = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_train_credit_application = pd.read_pickle('collected_train_credit_application')\n",
    "collected_test_credit_application = pd.read_pickle('collected_test_credit_application')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3 : Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_features = ['SK_ID_CURR', 'TARGET']\n",
    "\n",
    "all_features = list(set(collected_train_credit_application.columns.values.tolist()) - set(non_features))\n",
    "\n",
    "categorical_features = ['NAME_CONTRACT_TYPE', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', \n",
    "                        'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', \n",
    "                        'OCCUPATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE', \n",
    "                        'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', \n",
    "                        'EMERGENCYSTATE_MODE',\n",
    "                       ]\n",
    "\n",
    "flag_features = ['CODE_GENDER', \n",
    "                 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'FLAG_MOBIL', 'FLAG_EMP_PHONE', \n",
    "                 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL', \n",
    "                 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', \n",
    "                 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', \n",
    "                 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', \n",
    "                 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', \n",
    "                 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21',\n",
    "                 'REG_REGION_NOT_LIVE_REGIONREG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION', \n",
    "                 'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', \n",
    "                 'LIVE_CITY_NOT_WORK_CITY',\n",
    "                ]\n",
    "\n",
    "numerical_features = list(set(all_features) - set(non_features) - set(categorical_features) - set(flag_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_data(df, train_df=None):\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    new_df = df.copy()\n",
    "    new_train_df = train_df.copy() if train_df is not None else None\n",
    "    \n",
    "    median_comp_df = new_train_df if new_train_df is not None else new_df\n",
    "    for col in new_df.columns.values:\n",
    "        if new_df[col].dtype == np.float or new_df[col].dtype == np.int:\n",
    "            new_df[col] = new_df[col].fillna(median_comp_df[col].median())\n",
    "        elif new_df[col].dtype == np.object:\n",
    "            new_df[col] = new_df[col].fillna(median_comp_df[col].value_counts().idxmax())\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def handle_outlier(df):\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    new_df = df.copy()\n",
    "    \n",
    "    # new_df['SK_ID_CURR'] = new_df['SK_ID_CURR']\n",
    "    # if 'TARGET' in new_df:\n",
    "    #     new_df['TARGET'] = new_df['TARGET']\n",
    "    # new_df['NAME_CONTRACT_TYPE'] = new_df['NAME_CONTRACT_TYPE']\n",
    "    new_df['CODE_GENDER'] = new_df['CODE_GENDER'].apply(lambda val: 1 if val == 'XNA' or val == 'F' else 0)\n",
    "    new_df['FLAG_OWN_CAR'] = new_df['FLAG_OWN_CAR'].apply(lambda val: 1 if val == 'N' else 1)\n",
    "    new_df['FLAG_OWN_REALTY'] = new_df['FLAG_OWN_REALTY'].apply(lambda val: 1 if val == 'N' else 1)\n",
    "    new_df['CNT_CHILDREN'] = new_df['CNT_CHILDREN'].apply(lambda val: 3 if val > 3 else val)\n",
    "    new_df['AMT_INCOME_TOTAL'] = new_df['AMT_INCOME_TOTAL'].apply(lambda val: 500000 if val > 500000 else val)\n",
    "    new_df['AMT_CREDIT'] = new_df['AMT_CREDIT'].apply(lambda val: 1800000 if val > 1800000 else val)\n",
    "    new_df['AMT_ANNUITY'] = new_df['AMT_ANNUITY'].apply(lambda val: 100000 if val > 100000 else val)\n",
    "    new_df['AMT_GOODS_PRICE'] = new_df['AMT_GOODS_PRICE'].apply(lambda val: 2500000 if val > 2500000 else val)\n",
    "    # new_df['NAME_TYPE_SUITE'] = new_df['NAME_TYPE_SUITE']\n",
    "    # new_df['NAME_INCOME_TYPE'] = new_df['NAME_INCOME_TYPE']\n",
    "    # new_df['NAME_EDUCATION_TYPE'] = new_df['NAME_EDUCATION_TYPE']\n",
    "    new_df['NAME_FAMILY_STATUS'] = new_df['NAME_FAMILY_STATUS'].apply(lambda val: 'Married' if val == 'Unknown' else val)\n",
    "    # new_df['NAME_HOUSING_TYPE'] = new_df['NAME_HOUSING_TYPE']\n",
    "    # new_df['REGION_POPULATION_RELATIVE'] = new_df['REGION_POPULATION_RELATIVE']\n",
    "    # new_df['DAYS_BIRTH'] = new_df['DAYS_BIRTH']\n",
    "    new_df['DAYS_EMPLOYED'] = new_df['DAYS_EMPLOYED'].apply(lambda val: 0 if val > 0 else val)\n",
    "    new_df['DAYS_REGISTRATION'] = new_df['DAYS_REGISTRATION'].apply(lambda val: -18000 if val < -18000 else val)\n",
    "    new_df['DAYS_ID_PUBLISH'] = new_df['DAYS_ID_PUBLISH'].apply(lambda val: -6300 if val < -6300 else val)\n",
    "    new_df['OWN_CAR_AGE'] = new_df['OWN_CAR_AGE'].apply(lambda val: 65 if val > 65 else val)\n",
    "    # new_df['FLAG_MOBIL'] = new_df['FLAG_MOBIL']\n",
    "    # new_df['FLAG_EMP_PHONE'] = new_df['FLAG_EMP_PHONE']\n",
    "    # new_df['FLAG_WORK_PHONE'] = new_df['FLAG_WORK_PHONE']\n",
    "    # new_df['FLAG_CONT_MOBILE'] = new_df['FLAG_CONT_MOBILE']\n",
    "    # new_df['FLAG_PHONE'] = new_df['FLAG_PHONE']\n",
    "    # new_df['FLAG_EMAIL'] = new_df['FLAG_EMAIL']\n",
    "    # new_df['OCCUPATION_TYPE'] = new_df['OCCUPATION_TYPE']\n",
    "    # new_df['CNT_FAM_MEMBERS'] = new_df['CNT_FAM_MEMBERS']\n",
    "    # new_df['REGION_RATING_CLIENT'] = new_df['REGION_RATING_CLIENT']\n",
    "    # new_df['REGION_RATING_CLIENT_W_CITY'] = new_df['REGION_RATING_CLIENT_W_CITY']\n",
    "    # new_df['WEEKDAY_APPR_PROCESS_START'] = new_df['WEEKDAY_APPR_PROCESS_START']\n",
    "    # new_df['HOUR_APPR_PROCESS_START'] = new_df['HOUR_APPR_PROCESS_START']\n",
    "    # new_df['REG_REGION_NOT_LIVE_REGION'] = new_df['REG_REGION_NOT_LIVE_REGION']\n",
    "    # new_df['REG_REGION_NOT_WORK_REGION'] = new_df['REG_REGION_NOT_WORK_REGION']\n",
    "    # new_df['LIVE_REGION_NOT_WORK_REGION'] = new_df['LIVE_REGION_NOT_WORK_REGION']\n",
    "    # new_df['REG_CITY_NOT_LIVE_CITY'] = new_df['REG_CITY_NOT_LIVE_CITY']\n",
    "    # new_df['REG_CITY_NOT_WORK_CITY'] = new_df['REG_CITY_NOT_WORK_CITY']\n",
    "    # new_df['LIVE_CITY_NOT_WORK_CITY'] = new_df['LIVE_CITY_NOT_WORK_CITY']\n",
    "    new_df['ORGANIZATION_TYPE'] = new_df['ORGANIZATION_TYPE'].apply(lambda val: 'Unknown' if val == 'XNA' else val)\n",
    "    # new_df['EXT_SOURCE_1'] = new_df['EXT_SOURCE_1']\n",
    "    # new_df['EXT_SOURCE_2'] = new_df['EXT_SOURCE_2']\n",
    "    # new_df['EXT_SOURCE_3'] = new_df['EXT_SOURCE_3']\n",
    "    # new_df['APARTMENTS_AVG'] = new_df['APARTMENTS_AVG']\n",
    "    new_df['BASEMENTAREA_AVG'] = new_df['BASEMENTAREA_AVG'].apply(lambda val: 0.5 if val > 0.5 else val)\n",
    "    new_df['YEARS_BEGINEXPLUATATION_AVG'] = new_df['YEARS_BEGINEXPLUATATION_AVG'].apply(lambda val: 0.9 if val < 0.9 else val)\n",
    "    # new_df['YEARS_BUILD_AVG'] = new_df['YEARS_BUILD_AVG']\n",
    "    new_df['COMMONAREA_AVG'] = new_df['COMMONAREA_AVG'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['ELEVATORS_AVG'] = new_df['ELEVATORS_AVG'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['ENTRANCES_AVG'] = new_df['ENTRANCES_AVG'].apply(lambda val: 0.5 if val > 0.5 else val)\n",
    "    # new_df['FLOORSMAX_AVG'] = new_df['FLOORSMAX_AVG']\n",
    "    # new_df['FLOORSMIN_AVG'] = new_df['FLOORSMIN_AVG']\n",
    "    new_df['LANDAREA_AVG'] = new_df['LANDAREA_AVG'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['LIVINGAPARTMENTS_AVG'] = new_df['LIVINGAPARTMENTS_AVG'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['LIVINGAREA_AVG'] = new_df['LIVINGAREA_AVG'].apply(lambda val: 0.75 if val > 0.75 else val)\n",
    "    new_df['NONLIVINGAPARTMENTS_AVG'] = new_df['NONLIVINGAPARTMENTS_AVG'].apply(lambda val: 0.075 if val > 0.075 else val)\n",
    "    new_df['NONLIVINGAREA_AVG'] = new_df['NONLIVINGAREA_AVG'].apply(lambda val: 0.3 if val > 0.3 else val)\n",
    "    new_df['APARTMENTS_MODE'] = new_df['APARTMENTS_MODE'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['BASEMENTAREA_MODE'] = new_df['BASEMENTAREA_MODE'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['YEARS_BEGINEXPLUATATION_MODE'] = new_df['YEARS_BEGINEXPLUATATION_MODE'].apply(lambda val: 0.95 if val < 0.95 else val)\n",
    "    new_df['YEARS_BUILD_MODE'] = new_df['YEARS_BUILD_MODE'].apply(lambda val: 0.3 if val < 0.3 else val)\n",
    "    new_df['COMMONAREA_MODE'] = new_df['COMMONAREA_MODE'].apply(lambda val: 0.25 if val > 0.25 else val)\n",
    "    new_df['ELEVATORS_MODE'] = new_df['ELEVATORS_MODE'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['ENTRANCES_MODE'] = new_df['ENTRANCES_MODE'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['FLOORSMAX_MODE'] = new_df['FLOORSMAX_MODE'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['FLOORSMIN_MODE'] = new_df['FLOORSMIN_MODE'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['LANDAREA_MODE'] = new_df['LANDAREA_MODE'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['LIVINGAPARTMENTS_MODE'] = new_df['LIVINGAPARTMENTS_MODE'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['LIVINGAREA_MODE'] = new_df['LIVINGAREA_MODE'].apply(lambda val: 0.75 if val > 0.75 else val)\n",
    "    new_df['NONLIVINGAPARTMENTS_MODE'] = new_df['NONLIVINGAPARTMENTS_MODE'].apply(lambda val: 0.075 if val > 0.075 else val)\n",
    "    new_df['NONLIVINGAREA_MODE'] = new_df['NONLIVINGAREA_MODE'].apply(lambda val: 0.2 if val > 0.2 else val)\n",
    "    new_df['APARTMENTS_MEDI'] = new_df['APARTMENTS_MEDI'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['BASEMENTAREA_MEDI'] = new_df['BASEMENTAREA_MEDI'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['YEARS_BEGINEXPLUATATION_MEDI'] = new_df['YEARS_BEGINEXPLUATATION_MEDI'].apply(lambda val: 0.95 if val < 0.95 else val)\n",
    "    new_df['YEARS_BUILD_MEDI'] = new_df['YEARS_BUILD_MEDI'].apply(lambda val: 0.3 if val < 0.3 else val)\n",
    "    new_df['COMMONAREA_MEDI'] = new_df['COMMONAREA_MEDI'].apply(lambda val: 0.25 if val > 0.25 else val)\n",
    "    new_df['ELEVATORS_MEDI'] = new_df['ELEVATORS_MEDI'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['ENTRANCES_MEDI'] = new_df['ENTRANCES_MEDI'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['FLOORSMAX_MEDI'] = new_df['FLOORSMAX_MEDI'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['FLOORSMIN_MEDI'] = new_df['FLOORSMIN_MEDI'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['LANDAREA_MEDI'] = new_df['LANDAREA_MEDI'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['LIVINGAPARTMENTS_MEDI'] = new_df['LIVINGAPARTMENTS_MEDI'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['LIVINGAREA_MEDI'] = new_df['LIVINGAREA_MEDI'].apply(lambda val: 0.75 if val > 0.75 else val)\n",
    "    new_df['NONLIVINGAPARTMENTS_MEDI'] = new_df['NONLIVINGAPARTMENTS_MEDI'].apply(lambda val: 0.075 if val > 0.075 else val)\n",
    "    new_df['NONLIVINGAREA_MEDI'] = new_df['NONLIVINGAREA_MEDI'].apply(lambda val: 0.2 if val > 0.2 else val)\n",
    "    # new_df['FONDKAPREMONT_MODE'] = new_df['FONDKAPREMONT_MODE']\n",
    "    # new_df['HOUSETYPE_MODE'] = new_df['HOUSETYPE_MODE']\n",
    "    new_df['TOTALAREA_MODE'] = new_df['TOTALAREA_MODE'].apply(lambda val: 0.60 if val > 0.60 else val)\n",
    "    # new_df['WALLSMATERIAL_MODE'] = new_df['WALLSMATERIAL_MODE']\n",
    "    # new_df['EMERGENCYSTATE_MODE'] = new_df['EMERGENCYSTATE_MODE']\n",
    "    new_df['OBS_30_CNT_SOCIAL_CIRCLE'] = new_df['OBS_30_CNT_SOCIAL_CIRCLE'].apply(lambda val: 25 if val > 25 else val)\n",
    "    new_df['DEF_30_CNT_SOCIAL_CIRCLE'] = new_df['DEF_30_CNT_SOCIAL_CIRCLE'].apply(lambda val: 5 if val > 5 else val)\n",
    "    new_df['OBS_60_CNT_SOCIAL_CIRCLE'] = new_df['OBS_60_CNT_SOCIAL_CIRCLE'].apply(lambda val: 15 if val > 15 else val)\n",
    "    new_df['DEF_60_CNT_SOCIAL_CIRCLE'] = new_df['DEF_60_CNT_SOCIAL_CIRCLE'].apply(lambda val: 3 if val > 3 else val)\n",
    "    new_df['DAYS_LAST_PHONE_CHANGE'] = new_df['DAYS_LAST_PHONE_CHANGE'].apply(lambda val: -3200 if val < -3200 else val)\n",
    "    # new_df['FLAG_DOCUMENT_2'] = new_df['FLAG_DOCUMENT_2']\n",
    "    # new_df['FLAG_DOCUMENT_3'] = new_df['FLAG_DOCUMENT_3']\n",
    "    # new_df['FLAG_DOCUMENT_4'] = new_df['FLAG_DOCUMENT_4']\n",
    "    # new_df['FLAG_DOCUMENT_5'] = new_df['FLAG_DOCUMENT_5']\n",
    "    # new_df['FLAG_DOCUMENT_6'] = new_df['FLAG_DOCUMENT_6']\n",
    "    # new_df['FLAG_DOCUMENT_7'] = new_df['FLAG_DOCUMENT_7']\n",
    "    # new_df['FLAG_DOCUMENT_8'] = new_df['FLAG_DOCUMENT_8']\n",
    "    # new_df['FLAG_DOCUMENT_9'] = new_df['FLAG_DOCUMENT_9']\n",
    "    # new_df['FLAG_DOCUMENT_10'] = new_df['FLAG_DOCUMENT_10']\n",
    "    # new_df['FLAG_DOCUMENT_11'] = new_df['FLAG_DOCUMENT_11']\n",
    "    # new_df['FLAG_DOCUMENT_12'] = new_df['FLAG_DOCUMENT_12']\n",
    "    # new_df['FLAG_DOCUMENT_13'] = new_df['FLAG_DOCUMENT_13']\n",
    "    # new_df['FLAG_DOCUMENT_14'] = new_df['FLAG_DOCUMENT_14']\n",
    "    # new_df['FLAG_DOCUMENT_15'] = new_df['FLAG_DOCUMENT_15']\n",
    "    # new_df['FLAG_DOCUMENT_16'] = new_df['FLAG_DOCUMENT_16']\n",
    "    # new_df['FLAG_DOCUMENT_17'] = new_df['FLAG_DOCUMENT_17']\n",
    "    # new_df['FLAG_DOCUMENT_18'] = new_df['FLAG_DOCUMENT_18']\n",
    "    # new_df['FLAG_DOCUMENT_19'] = new_df['FLAG_DOCUMENT_19']\n",
    "    # new_df['FLAG_DOCUMENT_20'] = new_df['FLAG_DOCUMENT_20']\n",
    "    # new_df['FLAG_DOCUMENT_21'] = new_df['FLAG_DOCUMENT_21']\n",
    "    new_df['AMT_REQ_CREDIT_BUREAU_HOUR'] = new_df['AMT_REQ_CREDIT_BUREAU_HOUR'].apply(lambda val: 2.0 if val > 2.0 else val)\n",
    "    new_df['AMT_REQ_CREDIT_BUREAU_DAY'] = new_df['AMT_REQ_CREDIT_BUREAU_DAY'].apply(lambda val: 4.0 if val > 4.0 else val)\n",
    "    new_df['AMT_REQ_CREDIT_BUREAU_WEEK'] = new_df['AMT_REQ_CREDIT_BUREAU_WEEK'].apply(lambda val: 3.0 if val > 3.0 else val)\n",
    "    new_df['AMT_REQ_CREDIT_BUREAU_MON'] = new_df['AMT_REQ_CREDIT_BUREAU_MON'].apply(lambda val: 17.0 if val > 17.0 else val)\n",
    "    new_df['AMT_REQ_CREDIT_BUREAU_QRT'] = new_df['AMT_REQ_CREDIT_BUREAU_QRT'].apply(lambda val: 6.0 if val > 6.0 else val)\n",
    "    new_df['AMT_REQ_CREDIT_BUREAU_YEAR'] = new_df['AMT_REQ_CREDIT_BUREAU_YEAR'].apply(lambda val: 14.0 if val > 14.0 else val)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def clean_data(df, train_df=None):  \n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    new_df = df.copy()\n",
    "    new_train_df = train_df.copy() if train_df is not None else None\n",
    "    \n",
    "    # Missing Values\n",
    "    new_df = fill_missing_data(new_df, new_train_df)\n",
    "    new_train_df = fill_missing_data(new_train_df) if new_train_df is not None else None\n",
    "    \n",
    "    # Handling Outliers\n",
    "    new_df = handle_outlier(new_df)\n",
    "    new_train_df = handle_outlier(new_train_df) if new_train_df is not None else None\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_train_credit_application = clean_data(df=collected_train_credit_application)\n",
    "clean_test_credit_application = clean_data(df=collected_test_credit_application, train_df=collected_train_credit_application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4 : Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_feature(df, train_df=None):\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    new_df = df.copy()\n",
    "    if train_df is not None:\n",
    "        new_train_df = train_df.copy()\n",
    "    else:\n",
    "        new_train_df = None\n",
    "\n",
    "    # Dimensionality Reduction - Numerical Features\n",
    "    lda = discriminant_analysis.LinearDiscriminantAnalysis(n_components=2)\n",
    "    fitting_df = new_train_df.copy() if new_train_df is not None else new_df.copy()\n",
    "    fitting_targets = fitting_df['TARGET']\n",
    "    fitting_df = fitting_df.drop(columns=['SK_ID_CURR'])\n",
    "    fitting_df = fitting_df.drop(columns=['TARGET'])\n",
    "    lda.fit(fitting_df[numerical_features], fitting_targets)\n",
    "    new_df['lda_1'] = lda.transform(new_df[numerical_features])\n",
    "\n",
    "    # New Features\n",
    "    new_df['amt_income_per_member'] = new_df['AMT_INCOME_TOTAL'] / new_df['CNT_FAM_MEMBERS']\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_train_credit_application = engineer_feature(clean_train_credit_application)\n",
    "complete_test_credit_application = engineer_feature(clean_test_credit_application, train_df=clean_train_credit_application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5 : Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feature(df):\n",
    "    features = [\n",
    "#         'all',\n",
    "        \n",
    "#         'amt_income_per_member',\n",
    "        'lda_1',\n",
    "        \n",
    "#         'NAME_CONTRACT_TYPE',\n",
    "#         'CODE_GENDER',\n",
    "#         'FLAG_OWN_CAR',\n",
    "#         'FLAG_OWN_REALTY',\n",
    "#         'CNT_CHILDREN',\n",
    "#         'AMT_INCOME_TOTAL',\n",
    "#         'AMT_ANNUITY',\n",
    "#         'AMT_GOODS_PRICE',\n",
    "#         'NAME_TYPE_SUITE',\n",
    "#         'NAME_INCOME_TYPE',\n",
    "#         'NAME_EDUCATION_TYPE',\n",
    "#         'NAME_FAMILY_STATUS',\n",
    "#         'NAME_HOUSING_TYPE',\n",
    "#         'REGION_POPULATION_RELATIVE',\n",
    "#         'DAYS_BIRTH',\n",
    "#         'DAYS_EMPLOYED',\n",
    "#         'DAYS_REGISTRATION',\n",
    "#         'DAYS_ID_PUBLISH',\n",
    "#         'OWN_CAR_AGE',\n",
    "#         'OCCUPATION_TYPE',\n",
    "#         'REGION_RATING_CLIENT',\n",
    "#         'REGION_RATING_CLIENT_W_CITY',\n",
    "#         'WEEKDAY_APPR_PROCESS_START',\n",
    "#         'HOUR_APPR_PROCESS_START',\n",
    "#         'ORGANIZATION_TYPE',\n",
    "        'EXT_SOURCE_1',\n",
    "#         'EXT_SOURCE_2',\n",
    "#         'EXT_SOURCE_3',\n",
    "#         'OBS_30_CNT_SOCIAL_CIRCLE',\n",
    "#         'DEF_30_CNT_SOCIAL_CIRCLE',\n",
    "#         'OBS_60_CNT_SOCIAL_CIRCLE',\n",
    "#         'DEF_60_CNT_SOCIAL_CIRCLE',\n",
    "#         'DAYS_LAST_PHONE_CHANGE',\n",
    "#         'BUR_credit_active',\n",
    "#         'BUR_credit_closed',\n",
    "#         'BUR_credit_sold',\n",
    "#         'BUR_credit_bad_debt',\n",
    "#         'BUR_sum_credit_day_overdue',\n",
    "#         'BUR_mean_credit_day_overdue',\n",
    "#         'BUR_credit_type_count',\n",
    "#         'BUR_mean_days_credit_update',\n",
    "#         'PREV_application_count',\n",
    "#         'PREV_name_contract_type_count',\n",
    "#         'PREV_mean_amount_credit',\n",
    "#         'PREV_name_contract_status_approved_count',\n",
    "    ]\n",
    "\n",
    "    if 'all' in features:\n",
    "        new_df = df.copy()\n",
    "    else:\n",
    "        new_df = pd.DataFrame()\n",
    "\n",
    "        new_df['SK_ID_CURR'] = df['SK_ID_CURR']\n",
    "        if 'TARGET' in df:\n",
    "            new_df['TARGET'] = df['TARGET']\n",
    "            \n",
    "        for basic_feature in features:\n",
    "            features_contain_basic = [feature for feature in df if basic_feature in feature]\n",
    "            for feature in features_contain_basic:\n",
    "                new_df[feature] = df[feature]\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_credit_application = select_feature(complete_train_credit_application)\n",
    "test_credit_application = select_feature(complete_test_credit_application)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_credit_application.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6 : Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_categorical_data(df):  \n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    new_df = df.copy()\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        if col in new_df:\n",
    "            new_df = pd.get_dummies(new_df, columns=[col])\n",
    "            \n",
    "    # TODO: this is workaround for test data\n",
    "    if any('NAME_INCOME_TYPE' in col for col in new_df) and 'NAME_INCOME_TYPE_Maternity leave' not in new_df:\n",
    "        new_df['NAME_INCOME_TYPE_Maternity leave'] = 0\n",
    "            \n",
    "    return new_df\n",
    "\n",
    "def handle_numerical_data(df, train_df=None):\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    new_df = df.copy()\n",
    "    new_train_df = train_df.copy() if train_df is not None else None\n",
    "    \n",
    "    scale_df = new_train_df if new_train_df is not None else new_df\n",
    "    \n",
    "    for feature in numerical_features:\n",
    "        if feature in new_df and (scale_df[feature].dtype == np.float64 or scale_df[feature].dtype == np.int64):\n",
    "            stdsc = (preprocessing.StandardScaler()).fit(scale_df[feature].values.reshape(-1, 1))\n",
    "            new_df[feature] = stdsc.transform(new_df[feature].values.reshape(-1, 1))\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def preprocess(df, train_df=None):\n",
    "    new_df = df.copy()\n",
    "    new_train_df = train_df.copy() if train_df is not None else None\n",
    "    \n",
    "    # Categorical Data\n",
    "    new_df = handle_categorical_data(new_df)\n",
    "    \n",
    "    # Numerical Data\n",
    "    new_df = handle_numerical_data(new_df, new_train_df)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_credit_application = preprocess(train_credit_application)\n",
    "test_credit_application = preprocess(test_credit_application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 7 : Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_credit_application = train_credit_application.reindex(np.random.permutation(train_credit_application.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = train_credit_application['SK_ID_CURR'].count()\n",
    "training_count = math.ceil(total_count * 0.75)\n",
    "validation_count = math.floor(total_count * 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_credit_application = train_credit_application.head(training_count)\n",
    "validation_credit_application = train_credit_application.tail(validation_count)\n",
    "testing_credit_application = test_credit_application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ids_data_targets(df, dataframe=False):\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    new_df_ids = new_df['SK_ID_CURR']\n",
    "    if 'TARGET' in new_df:\n",
    "        new_df_targets = new_df['TARGET']\n",
    "        new_df_data = new_df.drop(columns=['SK_ID_CURR', 'TARGET'])\n",
    "    else:\n",
    "        new_df_targets = None\n",
    "        new_df_data = new_df.drop(columns=['SK_ID_CURR'])\n",
    "    \n",
    "    \n",
    "    if not dataframe:\n",
    "        new_df_ids = new_df_ids.values\n",
    "        new_df_data = new_df_data.values\n",
    "        if new_df_targets is not None:\n",
    "            new_df_targets = new_df_targets.values\n",
    "    \n",
    "    return new_df_ids, new_df_data, new_df_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1, temp2, temp3 = split_ids_data_targets(training_credit_application, dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ids, training_data, training_targets = split_ids_data_targets(training_credit_application)\n",
    "validation_ids, validation_data, validation_targets = split_ids_data_targets(validation_credit_application)\n",
    "testing_ids, testing_data, testing_targets = split_ids_data_targets(testing_credit_application)\n",
    "\n",
    "training_targets_onehot = (preprocessing.OneHotEncoder().fit_transform(training_targets.reshape(-1, 1))).toarray()\n",
    "validation_targets_onehot = (preprocessing.OneHotEncoder().fit_transform(validation_targets.reshape(-1, 1))).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 8 : Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model, ensemble, svm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd_classifier =  linear_model.SGDClassifier()\n",
    "sgd_classifier.fit(training_data, training_targets)\n",
    "\n",
    "training_predictions = sgd_classifier.predict(training_data)\n",
    "validation_predictions = sgd_classifier.predict(validation_data)\n",
    "\n",
    "print('Training   - acc: %.4f, auc: %.4f' % (metrics.accuracy_score(training_targets, training_predictions),\n",
    "                                             metrics.roc_auc_score(training_targets, training_predictions)))\n",
    "print('Validation - acc: %.4f, auc: %.4f' % (metrics.accuracy_score(validation_targets, validation_predictions),\n",
    "                                             metrics.roc_auc_score(validation_targets, validation_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = linear_model.LogisticRegression(C=2)\n",
    "logistic_regression.fit(training_data, training_targets)\n",
    "\n",
    "training_predictions = logistic_regression.predict(training_data)\n",
    "validation_predictions = logistic_regression.predict(validation_data)\n",
    "\n",
    "print('Training   - acc: %.4f, auc: %.4f' % (metrics.accuracy_score(training_targets, training_predictions),\n",
    "                                             metrics.roc_auc_score(training_targets, training_predictions)))\n",
    "print('Validation - acc: %.4f, auc: %.4f' % (metrics.accuracy_score(validation_targets, validation_predictions),\n",
    "                                             metrics.roc_auc_score(validation_targets, validation_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "svm_classifier = svm.SVC()\n",
    "svm_classifier.fit(training_data, training_targets)\n",
    "\n",
    "training_predictions = svm_classifier.predict(training_data)\n",
    "validation_predictions = svm_classifier.predict(validation_data)\n",
    "\n",
    "print('Training   - acc: %.4f, auc: %.4f' % (metrics.accuracy_score(training_targets, training_predictions),\n",
    "                                             metrics.roc_auc_score(training_targets, training_predictions)))\n",
    "print('Validation - acc: %.4f, auc: %.4f' % (metrics.accuracy_score(validation_targets, validation_predictions),\n",
    "                                             metrics.roc_auc_score(validation_targets, validation_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier = tree.DecisionTreeClassifier(max_depth=10, criterion='gini')\n",
    "tree_classifier.fit(training_data, training_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions = tree_classifier.predict(training_data)\n",
    "validation_predictions = tree_classifier.predict(validation_data)\n",
    "\n",
    "print('Training   - acc: %.4f, auc: %.4f' % (metrics.accuracy_score(training_targets, training_predictions),\n",
    "                                             metrics.roc_auc_score(training_targets, training_predictions)))\n",
    "print('Validation - acc: %.4f, auc: %.4f' % (metrics.accuracy_score(validation_targets, validation_predictions),\n",
    "                                             metrics.roc_auc_score(validation_targets, validation_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in enumerate(tree_classifier.feature_importances_.tolist()):\n",
    "    print('%s : %f' % (temp2.columns.values.tolist()[index], value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_classifier = ensemble.RandomForestClassifier(n_estimators=30, criterion='gini', verbose=1)\n",
    "rf_classifier.fit(training_data, training_targets)\n",
    "\n",
    "training_predictions = rf_classifier.predict(training_data)\n",
    "validation_predictions = rf_classifier.predict(validation_data)\n",
    "\n",
    "print('Training   - acc: %.4f, auc: %.4f' % (metrics.accuracy_score(training_targets, training_predictions),\n",
    "                                             metrics.roc_auc_score(training_targets, training_predictions)))\n",
    "print('Validation - acc: %.4f, auc: %.4f' % (metrics.accuracy_score(validation_targets, validation_predictions),\n",
    "                                             metrics.roc_auc_score(validation_targets, validation_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "gb_classifier = ensemble.GradientBoostingClassifier()\n",
    "gb_classifier.fit(training_data, training_targets)\n",
    "\n",
    "training_predictions = gb_classifier.predict(training_data)\n",
    "validation_predictions = gb_classifier.predict(validation_data)\n",
    "\n",
    "print('Training   - acc: %.4f, auc: %.4f' % (metrics.accuracy_score(training_targets, training_predictions),\n",
    "                                             metrics.roc_auc_score(training_targets, training_predictions)))\n",
    "print('Validation - acc: %.4f, auc: %.4f' % (metrics.accuracy_score(validation_targets, validation_predictions),\n",
    "                                             metrics.roc_auc_score(validation_targets, validation_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_metric(y_true, y_pred):\n",
    "    return tf.Variable(metrics.roc_auc_score(y_true, y_pred), name='auc_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_activation = 'sigmoid'\n",
    "default_last_activation = 'sigmoid'\n",
    "default_batch_size = 1000\n",
    "default_epochs = 30\n",
    "\n",
    "dnn_classifier_settings = []\n",
    "\n",
    "default_dnn_classifier_settings = [\n",
    "#     # Adam\n",
    "#     {'optimizer': optimizers.Adam(), 'batch_size': default_batch_size, 'epochs': default_epochs, \n",
    "#      'activation': default_activation, 'last_activation': default_last_activation,\n",
    "#      'acc': None, 'val_acc': None, 'history': None, 'auc': None, 'val_auc': None,\n",
    "#      'training_predictions_onehot': None, 'validation_predictions_onehot': None},\n",
    "    \n",
    "# #     # SGD\n",
    "# #     {'optimizer': optimizers.SGD(momentum=0.1), 'batch_size': default_batch_size, 'epochs': default_epochs * 10,\n",
    "# #      'activation': default_activation, 'last_activation': default_last_activation,\n",
    "# #      'acc': None, 'val_acc': None, 'history': None, 'auc': None, 'val_auc': None,\n",
    "# #      'training_predictions': None, 'validation_predictions': None},\n",
    "    \n",
    "#     # Adagrad\n",
    "#     {'optimizer': optimizers.Adagrad(), 'batch_size': default_batch_size, 'epochs': default_epochs, \n",
    "#      'activation': default_activation, 'last_activation': default_last_activation,\n",
    "#      'acc': None, 'val_acc': None, 'history': None, 'auc': None, 'val_auc': None,\n",
    "#      'training_predictions': None, 'validation_predictions': None},\n",
    "    \n",
    "#     # RMSprop\n",
    "#     {'optimizer': optimizers.RMSprop(), 'batch_size': default_batch_size, 'epochs': default_epochs, \n",
    "#      'activation': default_activation, 'last_activation': default_last_activation,\n",
    "#      'acc': None, 'val_acc': None, 'history': None, 'auc': None, 'val_auc': None,\n",
    "#      'training_predictions': None, 'validation_predictions': None},\n",
    "    \n",
    "#     # Adamax\n",
    "#     {'optimizer': optimizers.Adamax(), 'batch_size': default_batch_size, 'epochs': default_epochs, \n",
    "#      'activation': default_activation, 'last_activation': default_last_activation,\n",
    "#      'acc': None, 'val_acc': None, 'history': None, 'auc': None, 'val_auc': None,\n",
    "#      'training_predictions': None, 'validation_predictions': None},\n",
    "    \n",
    "#     # Nadam\n",
    "#     {'optimizer': optimizers.Nadam(), 'batch_size': default_batch_size, 'epochs': default_epochs, \n",
    "#      'activation': default_activation, 'last_activation': default_last_activation,\n",
    "#      'acc': None, 'val_acc': None, 'history': None, 'auc': None, 'val_auc': None,\n",
    "#      'training_predictions': None, 'validation_predictions': None},\n",
    "]\n",
    "\n",
    "dnn_classifier_settings.extend(default_dnn_classifier_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_settings = [\n",
    "    # Adam\n",
    "    {'optimizer': optimizers.Adamax(lr=0.0001), 'batch_size': default_batch_size, 'epochs': default_epochs, \n",
    "     'activation': default_activation, 'last_activation': default_last_activation,\n",
    "     'acc': None, 'val_acc': None, 'history': None, 'auc': None, 'val_auc': None,\n",
    "     'training_predictions_onehot': None, 'validation_predictions_onehot': None},\n",
    "    {'optimizer': optimizers.Adamax(lr=0.001), 'batch_size': default_batch_size, 'epochs': default_epochs, \n",
    "     'activation': default_activation, 'last_activation': default_last_activation,\n",
    "     'acc': None, 'val_acc': None, 'history': None, 'auc': None, 'val_auc': None,\n",
    "     'training_predictions_onehot': None, 'validation_predictions_onehot': None},\n",
    "    {'optimizer': optimizers.Adamax(lr=0.01), 'batch_size': default_batch_size, 'epochs': default_epochs, \n",
    "     'activation': default_activation, 'last_activation': default_last_activation,\n",
    "     'acc': None, 'val_acc': None, 'history': None, 'auc': None, 'val_auc': None,\n",
    "     'training_predictions_onehot': None, 'validation_predictions_onehot': None},\n",
    "]\n",
    "\n",
    "if len(new_settings) > 0:\n",
    "    dnn_classifier_settings.extend(new_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, dnn_classifier_setting in enumerate(dnn_classifier_settings):\n",
    "    if dnn_classifier_setting['acc'] is None and dnn_classifier_setting['val_acc'] is None and dnn_classifier_setting['history'] is None:\n",
    "        optimizer = dnn_classifier_setting['optimizer']\n",
    "        batch_size = dnn_classifier_setting['batch_size']\n",
    "        epochs = dnn_classifier_setting['epochs']\n",
    "        activation = dnn_classifier_setting['activation']\n",
    "        last_activation = dnn_classifier_setting['last_activation']\n",
    "\n",
    "        dnn_classifier = Sequential()\n",
    "        input_shape = (training_data.shape[1], )\n",
    "        dnn_classifier.add(Dense(128, activation=activation, input_shape=input_shape))\n",
    "        # dnn_classifier.add(Dropout(rate=0.35))\n",
    "        dnn_classifier.add(Dense(128, activation=activation))\n",
    "        # dnn_classifier.add(Dropout(rate=0.35))\n",
    "        dnn_classifier.add(Dense(64, activation=activation))\n",
    "        # dnn_classifier.add(Dropout(rate=0.25))\n",
    "        dnn_classifier.add(Dense(2, activation=last_activation))\n",
    "        dnn_classifier.compile(loss='binary_crossentropy', \n",
    "                               optimizer=optimizer,\n",
    "                               metrics=['acc'])\n",
    "        history = dnn_classifier.fit(training_data, training_targets_onehot,\n",
    "                          epochs=epochs, batch_size=batch_size, verbose=False, shuffle=True,\n",
    "                          validation_data=(validation_data, validation_targets_onehot))\n",
    "\n",
    "        dnn_classifier_setting['history'] = history\n",
    "        \n",
    "        training_predictions_onehot = dnn_classifier.predict(training_data)\n",
    "        training_predictions = pd.DataFrame(training_predictions_onehot).apply(lambda val: 1.0 if val[1] > 0.3 else 0.0, axis=1)\n",
    "        \n",
    "        validation_predictions_onehot = dnn_classifier.predict(validation_data)\n",
    "        validation_predictions = pd.DataFrame(validation_predictions_onehot).apply(lambda val: 1.0 if val[1] > 0.3 else 0.0, axis=1)\n",
    "        \n",
    "        dnn_classifier_setting['training_predictions_onehot'] = training_predictions_onehot\n",
    "        dnn_classifier_setting['validation_predictions_onehot'] = validation_predictions_onehot\n",
    "        dnn_classifier_setting['acc'] = metrics.accuracy_score(training_targets, training_predictions)\n",
    "        dnn_classifier_setting['val_acc'] = metrics.accuracy_score(validation_targets, validation_predictions)\n",
    "        dnn_classifier_setting['auc'] = metrics.roc_auc_score(training_targets, training_predictions)\n",
    "        dnn_classifier_setting['val_auc'] = metrics.roc_auc_score(validation_targets, validation_predictions)\n",
    "\n",
    "    print('%2d: Optimizer: %10s; LR: %.5f; bs: %3d; epochs: %4d; acc: %.4f; val_acc: %.4f; auc: %.2f; val_auc: %.2f' % (index, \n",
    "                                                                                              type(dnn_classifier_setting['optimizer']).__name__, \n",
    "                                                                                              dnn_classifier_setting['optimizer'].get_config()['lr'], \n",
    "                                                                                              dnn_classifier_setting['batch_size'], \n",
    "                                                                                              dnn_classifier_setting['epochs'],\n",
    "                                                                                              dnn_classifier_setting['acc'], \n",
    "                                                                                              dnn_classifier_setting['val_acc'],\n",
    "                                                                                              dnn_classifier_setting['auc'],\n",
    "                                                                                              dnn_classifier_setting['val_auc']))\n",
    "\n",
    "IPython.display.Audio('http://www.pacdv.com/sounds/interface_sound_effects/sound94.wav', autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "for dnn_classifier_setting in dnn_classifier_settings:\n",
    "    plt.plot(dnn_classifier_setting['history'].history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(15, 20))\n",
    "for index, setting in enumerate(dnn_classifier_settings):\n",
    "    plt.subplot(10, 2, index + 1)\n",
    "    plt.title('Config %d' % index)\n",
    "    pd.Series(setting['training_predictions_onehot'][:, 1]).hist(bins=100)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_range = np.linspace(0.01, 0.5, 10)\n",
    "\n",
    "for boundary in boundary_range:\n",
    "    training_predictions = dnn_classifier_settings[0]['training_predictions_onehot'] / boundary * 0.5\n",
    "    validation_predictions = dnn_classifier_settings[0]['validation_predictions_onehot'] / boundary * 0.5\n",
    "    training_predictions = pd.DataFrame(training_predictions).apply(lambda val: 1.0 if val[1] > 0.5 else 0.0, axis=1)\n",
    "    validation_predictions = pd.DataFrame(validation_predictions).apply(lambda val: 1.0 if val[1] > 0.5 else 0.0, axis=1)\n",
    "    print('boundary: %.4f, training AUC score: %.4f, val AUC score: %.4f' % (boundary, metrics.roc_auc_score(training_targets, training_predictions), metrics.roc_auc_score(validation_targets, validation_predictions)))\n",
    "    \n",
    "IPython.display.Audio('http://www.pacdv.com/sounds/interface_sound_effects/sound94.wav', autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 9 : Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier = dnn_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training_predictions = classifier.predict(training_data)\n",
    "print('Classifier: %s - acc: %.4f, auc: %.4f' % (classifier.__class__.__name__,\n",
    "                                                 metrics.accuracy_score(training_targets, training_predictions),\n",
    "                                                 metrics.roc_auc_score(training_targets, training_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training_predictions = classifier.predict(training_data)\n",
    "print(training_predictions)\n",
    "if type(classifier) == Sequential:\n",
    "    training_predictions = pd.DataFrame(training_predictions).apply(lambda val: 1.0 if val[1] > 0.1 else 0.0, axis=1)\n",
    "    validation_predictions = classifier.predict(validation_data)\n",
    "    validation_predictions = pd.DataFrame(validation_predictions).apply(lambda val: 1.0 if val[1] > 0.1 else 0.0, axis=1)\n",
    "else:\n",
    "    print(training_predictions.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(metrics.accuracy_score(training_targets, training_predictions))\n",
    "print(metrics.roc_auc_score(training_targets, training_predictions))\n",
    "print(metrics.accuracy_score(validation_targets, validation_predictions))\n",
    "print(metrics.roc_auc_score(validation_targets, validation_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 10 : Prepare Output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
