{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import IPython\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_credit_application = pd.read_csv('data/application_train.csv')\n",
    "raw_test_credit_application = pd.read_csv('data/application_test.csv')\n",
    "\n",
    "# raw_bureau = pd.read_csv('data/bureau.csv')\n",
    "# raw_bureau_balance = pd.read_csv('data/bureau_balance.csv')\n",
    "# raw_credit_card_balance = pd.read_csv('data/credit_card_balance.csv')\n",
    "# raw_installments_payments = pd.read_csv('data/installments_payments.csv')\n",
    "# raw_pos_cash_balance = pd.read_csv('data/pos_cash_balance.csv')\n",
    "# raw_previous_application = pd.read_csv('data/previous_application.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=[15, 100])\n",
    "columns = ['NAME_CONTRACT_TYPE', 'CODE_GENDER',\n",
    "           'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN',\n",
    "           'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',\n",
    "           'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'FLAG_MOBIL',\n",
    "           'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE',\n",
    "           'FLAG_PHONE', 'FLAG_EMAIL', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3',\n",
    "           'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6',\n",
    "           'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9',\n",
    "           'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12',\n",
    "           'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15',\n",
    "           'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18',\n",
    "           'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21'\n",
    "          ]\n",
    "row_no = 20\n",
    "column_no = 3\n",
    "for index, column in enumerate(columns):\n",
    "    plt.subplot(row_no, column_no, index + 1)\n",
    "    plt.title(column)\n",
    "    raw_train_credit_application.groupby([column]).TARGET.value_counts().plot(kind='bar')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_data(df):\n",
    "#     new_ids = df['SK_ID_CURR'].to_frame()\n",
    "#     new_data = pd.DataFrame()\n",
    "#     if 'TARGET' in df.columns:\n",
    "#         new_targets = df['TARGET'].to_frame()\n",
    "#     else:\n",
    "#         new_targets = None\n",
    "    \n",
    "#     # NAME_CONTRACT_TYPE\n",
    "#     new_data['NAME_CONTRACT_TYPE'] = df['NAME_CONTRACT_TYPE']\n",
    "    \n",
    "#     # CODE_GENDER\n",
    "#     new_data['CODE_GENDER'] = df['CODE_GENDER'].apply(lambda val: 'F' if val == 'XNA' else val)\n",
    "    \n",
    "#     # NAME_TYPE_SUITE\n",
    "#     new_data['NAME_TYPE_SUITE'] = df['NAME_TYPE_SUITE']\n",
    "    \n",
    "#     # CNT_CHILDREN\n",
    "#     new_data['CNT_CHILDREN'] = df['CNT_CHILDREN'].apply(lambda val: 3 if val > 3 else val)\n",
    "    \n",
    "#     # AMT\n",
    "#     new_data['AMT_CREDIT'] = df['AMT_CREDIT'].apply(lambda val: 1800000 if val > 1800000 else val)\n",
    "#     new_data['AMT_INCOME_TOTAL'] = df['AMT_INCOME_TOTAL'].apply(lambda val: 500000 if val > 500000 else val)\n",
    "#     new_data['AMT_ANNUITY'] = raw_train_credit_application['AMT_ANNUITY'].apply(lambda val: 100000 if val > 100000 else val)\n",
    "#     new_data['AMT_GOODS_PRICE'] = raw_train_credit_application['AMT_GOODS_PRICE'].apply(lambda val: 2500000 if val > 2500000 else val)\n",
    "\n",
    "#     # FLAGS\n",
    "#     # new_data['FLAG_OWN_CAR'] = df['FLAG_OWN_CAR'].apply(lambda val: '0' if val == 'N' else 1)\n",
    "#     # new_data['FLAG_OWN_REALTY'] = df['FLAG_OWN_REALTY'].apply(lambda val: '0' if val == 'N' else 1)\n",
    "#     # new_data['FLAG_MOBIL'] = df['FLAG_MOBIL']\n",
    "#     # new_data['FLAG_EMP_PHONE'] = df['FLAG_EMP_PHONE']\n",
    "#     # new_data['FLAG_WORK_PHONE'] = df['FLAG_WORK_PHONE']\n",
    "#     # new_data['FLAG_CONT_MOBILE'] = df['FLAG_CONT_MOBILE']\n",
    "#     # new_data['FLAG_PHONE'] = df['FLAG_PHONE']\n",
    "#     # new_data['FLAG_EMAIL'] = df['FLAG_EMAIL']\n",
    "#     # new_data['FLAG_DOCUMENT_2'] = df['FLAG_DOCUMENT_2']\n",
    "#     # new_data['FLAG_DOCUMENT_3'] = df['FLAG_DOCUMENT_3']\n",
    "#     # new_data['FLAG_DOCUMENT_4'] = df['FLAG_DOCUMENT_4']\n",
    "#     # new_data['FLAG_DOCUMENT_5'] = df['FLAG_DOCUMENT_5']\n",
    "#     # new_data['FLAG_DOCUMENT_6'] = df['FLAG_DOCUMENT_6']\n",
    "#     # new_data['FLAG_DOCUMENT_7'] = df['FLAG_DOCUMENT_7']\n",
    "#     # new_data['FLAG_DOCUMENT_8'] = df['FLAG_DOCUMENT_8']\n",
    "#     # new_data['FLAG_DOCUMENT_9'] = df['FLAG_DOCUMENT_9']\n",
    "#     # new_data['FLAG_DOCUMENT_10'] = df['FLAG_DOCUMENT_10']\n",
    "#     # new_data['FLAG_DOCUMENT_11'] = df['FLAG_DOCUMENT_11']\n",
    "#     # new_data['FLAG_DOCUMENT_12'] = df['FLAG_DOCUMENT_12']\n",
    "#     # new_data['FLAG_DOCUMENT_13'] = df['FLAG_DOCUMENT_13']\n",
    "#     # new_data['FLAG_DOCUMENT_14'] = df['FLAG_DOCUMENT_14']\n",
    "#     # new_data['FLAG_DOCUMENT_15'] = df['FLAG_DOCUMENT_15']\n",
    "#     # new_data['FLAG_DOCUMENT_16'] = df['FLAG_DOCUMENT_16']\n",
    "#     # new_data['FLAG_DOCUMENT_17'] = df['FLAG_DOCUMENT_17']\n",
    "#     # new_data['FLAG_DOCUMENT_18'] = df['FLAG_DOCUMENT_18']\n",
    "#     # new_data['FLAG_DOCUMENT_19'] = df['FLAG_DOCUMENT_19']\n",
    "#     # new_data['FLAG_DOCUMENT_20'] = df['FLAG_DOCUMENT_20']\n",
    "#     # new_data['FLAG_DOCUMENT_21'] = df['FLAG_DOCUMENT_21']\n",
    "    \n",
    "#     new_data = pd.get_dummies(new_data, columns=['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'NAME_TYPE_SUITE'])\n",
    "    \n",
    "#     return new_ids, new_data, new_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, train_df=None):     \n",
    "    new_df = df.copy()\n",
    "    \n",
    "    if train_df is None:\n",
    "        for col in new_df.columns.values:\n",
    "            if new_df[col].dtype == np.float or new_df[col].dtype == np.int:\n",
    "                new_df[col] = new_df[col].fillna(new_df[col].median())\n",
    "            elif new_df[col].dtype == np.object:\n",
    "                new_df[col] = new_df[col].fillna(new_df[col].value_counts().idxmax())\n",
    "    else:\n",
    "        for col in new_df.columns.values:\n",
    "            if new_df[col].dtype == np.float64 or new_df[col].dtype == np.int64:\n",
    "                new_df[col] = new_df[col].fillna(train_df[col].median())\n",
    "            elif new_df[col].dtype == np.object:\n",
    "                new_df[col] = new_df[col].fillna(train_df[col].value_counts().idxmax())\n",
    "    \n",
    "    new_df['SK_ID_CURR'] = new_df['SK_ID_CURR']\n",
    "    if 'TARGET' in new_df:\n",
    "        new_df['TARGET'] = new_df['TARGET']\n",
    "    new_df['NAME_CONTRACT_TYPE'] = new_df['NAME_CONTRACT_TYPE']\n",
    "    new_df['CODE_GENDER'] = new_df['CODE_GENDER'].apply(lambda val: 'F' if val == 'XNA' else val)\n",
    "    new_df['FLAG_OWN_CAR'] = new_df['FLAG_OWN_CAR'].apply(lambda val: '0' if val == 'N' else 1)\n",
    "    new_df['FLAG_OWN_REALTY'] = new_df['FLAG_OWN_REALTY'].apply(lambda val: '0' if val == 'N' else 1)\n",
    "    new_df['CNT_CHILDREN'] = new_df['CNT_CHILDREN'].apply(lambda val: 3 if val > 3 else val)\n",
    "    new_df['AMT_INCOME_TOTAL'] = new_df['AMT_INCOME_TOTAL'].apply(lambda val: 500000 if val > 500000 else val)\n",
    "    new_df['AMT_CREDIT'] = new_df['AMT_CREDIT'].apply(lambda val: 1800000 if val > 1800000 else val)\n",
    "    new_df['AMT_ANNUITY'] = new_df['AMT_ANNUITY'].apply(lambda val: 100000 if val > 100000 else val)\n",
    "    new_df['AMT_GOODS_PRICE'] = new_df['AMT_GOODS_PRICE'].apply(lambda val: 2500000 if val > 2500000 else val)\n",
    "    new_df['NAME_TYPE_SUITE'] = new_df['NAME_TYPE_SUITE']\n",
    "    new_df['NAME_INCOME_TYPE'] = new_df['NAME_INCOME_TYPE']\n",
    "    new_df['NAME_EDUCATION_TYPE'] = new_df['NAME_EDUCATION_TYPE']\n",
    "    new_df['NAME_FAMILY_STATUS'] = new_df['NAME_FAMILY_STATUS'].apply(lambda val: 'Married' if val == 'Unknown' else val)\n",
    "    new_df['NAME_HOUSING_TYPE'] = new_df['NAME_HOUSING_TYPE']\n",
    "    new_df['REGION_POPULATION_RELATIVE'] = new_df['REGION_POPULATION_RELATIVE']\n",
    "    new_df['DAYS_BIRTH'] = new_df['DAYS_BIRTH']\n",
    "    new_df['DAYS_EMPLOYED'] = new_df['DAYS_EMPLOYED'].apply(lambda val: 0 if val > 0 else val)\n",
    "    new_df['DAYS_REGISTRATION'] = new_df['DAYS_REGISTRATION'].apply(lambda val: -18000 if val < -18000 else val)\n",
    "    new_df['DAYS_ID_PUBLISH'] = new_df['DAYS_ID_PUBLISH'].apply(lambda val: -6300 if val < -6300 else val)\n",
    "    new_df['OWN_CAR_AGE'] = new_df['OWN_CAR_AGE'].apply(lambda val: 65 if val > 65 else val)\n",
    "    new_df['FLAG_MOBIL'] = new_df['FLAG_MOBIL']\n",
    "    new_df['FLAG_EMP_PHONE'] = new_df['FLAG_EMP_PHONE']\n",
    "    new_df['FLAG_WORK_PHONE'] = new_df['FLAG_WORK_PHONE']\n",
    "    new_df['FLAG_CONT_MOBILE'] = new_df['FLAG_CONT_MOBILE']\n",
    "    new_df['FLAG_PHONE'] = new_df['FLAG_PHONE']\n",
    "    new_df['FLAG_EMAIL'] = new_df['FLAG_EMAIL']\n",
    "    new_df['OCCUPATION_TYPE'] = new_df['OCCUPATION_TYPE']\n",
    "    new_df['CNT_FAM_MEMBERS'] = new_df['CNT_FAM_MEMBERS']\n",
    "    new_df['REGION_RATING_CLIENT'] = new_df['REGION_RATING_CLIENT']\n",
    "    new_df['REGION_RATING_CLIENT_W_CITY'] = new_df['REGION_RATING_CLIENT_W_CITY']\n",
    "    new_df['WEEKDAY_APPR_PROCESS_START'] = new_df['WEEKDAY_APPR_PROCESS_START']\n",
    "    new_df['HOUR_APPR_PROCESS_START'] = new_df['HOUR_APPR_PROCESS_START']\n",
    "    new_df['REG_REGION_NOT_LIVE_REGION'] = new_df['REG_REGION_NOT_LIVE_REGION']\n",
    "    new_df['REG_REGION_NOT_WORK_REGION'] = new_df['REG_REGION_NOT_WORK_REGION']\n",
    "    new_df['LIVE_REGION_NOT_WORK_REGION'] = new_df['LIVE_REGION_NOT_WORK_REGION']\n",
    "    new_df['REG_CITY_NOT_LIVE_CITY'] = new_df['REG_CITY_NOT_LIVE_CITY']\n",
    "    new_df['REG_CITY_NOT_WORK_CITY'] = new_df['REG_CITY_NOT_WORK_CITY']\n",
    "    new_df['LIVE_CITY_NOT_WORK_CITY'] = new_df['LIVE_CITY_NOT_WORK_CITY']\n",
    "    new_df['ORGANIZATION_TYPE'] = new_df['ORGANIZATION_TYPE'].apply(lambda val: 'Unknown' if val == 'XNA' else val)\n",
    "    new_df['EXT_SOURCE_1'] = new_df['EXT_SOURCE_1']\n",
    "    new_df['EXT_SOURCE_2'] = new_df['EXT_SOURCE_2']\n",
    "    new_df['EXT_SOURCE_3'] = new_df['EXT_SOURCE_3']\n",
    "    new_df['APARTMENTS_AVG'] = new_df['APARTMENTS_AVG']\n",
    "    new_df['BASEMENTAREA_AVG'] = new_df['BASEMENTAREA_AVG'].apply(lambda val: 0.5 if val > 0.5 else val)\n",
    "    new_df['YEARS_BEGINEXPLUATATION_AVG'] = new_df['YEARS_BEGINEXPLUATATION_AVG'].apply(lambda val: 0.9 if val < 0.9 else val)\n",
    "    new_df['YEARS_BUILD_AVG'] = new_df['YEARS_BUILD_AVG']\n",
    "    new_df['COMMONAREA_AVG'] = new_df['COMMONAREA_AVG'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['ELEVATORS_AVG'] = new_df['ELEVATORS_AVG'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['ENTRANCES_AVG'] = new_df['ENTRANCES_AVG'].apply(lambda val: 0.5 if val > 0.5 else val)\n",
    "    new_df['FLOORSMAX_AVG'] = new_df['FLOORSMAX_AVG']\n",
    "    new_df['FLOORSMIN_AVG'] = new_df['FLOORSMIN_AVG']\n",
    "    new_df['LANDAREA_AVG'] = new_df['LANDAREA_AVG'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['LIVINGAPARTMENTS_AVG'] = new_df['LIVINGAPARTMENTS_AVG'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['LIVINGAREA_AVG'] = new_df['LIVINGAREA_AVG'].apply(lambda val: 0.75 if val > 0.75 else val)\n",
    "    new_df['NONLIVINGAPARTMENTS_AVG'] = new_df['NONLIVINGAPARTMENTS_AVG'].apply(lambda val: 0.075 if val > 0.075 else val)\n",
    "    new_df['NONLIVINGAREA_AVG'] = new_df['NONLIVINGAREA_AVG'].apply(lambda val: 0.3 if val > 0.3 else val)\n",
    "    new_df['APARTMENTS_MODE'] = new_df['APARTMENTS_MODE'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['BASEMENTAREA_MODE'] = new_df['BASEMENTAREA_MODE'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['YEARS_BEGINEXPLUATATION_MODE'] = new_df['YEARS_BEGINEXPLUATATION_MODE'].apply(lambda val: 0.95 if val < 0.95 else val)\n",
    "    new_df['YEARS_BUILD_MODE'] = new_df['YEARS_BUILD_MODE'].apply(lambda val: 0.3 if val < 0.3 else val)\n",
    "    new_df['COMMONAREA_MODE'] = new_df['COMMONAREA_MODE'].apply(lambda val: 0.25 if val > 0.25 else val)\n",
    "    new_df['ELEVATORS_MODE'] = new_df['ELEVATORS_MODE'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['ENTRANCES_MODE'] = new_df['ENTRANCES_MODE'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['FLOORSMAX_MODE'] = new_df['FLOORSMAX_MODE'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['FLOORSMIN_MODE'] = new_df['FLOORSMIN_MODE'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['LANDAREA_MODE'] = new_df['LANDAREA_MODE'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['LIVINGAPARTMENTS_MODE'] = new_df['LIVINGAPARTMENTS_MODE'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['LIVINGAREA_MODE'] = new_df['LIVINGAREA_MODE'].apply(lambda val: 0.75 if val > 0.75 else val)\n",
    "    new_df['NONLIVINGAPARTMENTS_MODE'] = new_df['NONLIVINGAPARTMENTS_MODE'].apply(lambda val: 0.075 if val > 0.075 else val)\n",
    "    new_df['NONLIVINGAREA_MODE'] = new_df['NONLIVINGAREA_MODE'].apply(lambda val: 0.2 if val > 0.2 else val)\n",
    "    new_df['APARTMENTS_MEDI'] = new_df['APARTMENTS_MEDI'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['BASEMENTAREA_MEDI'] = new_df['BASEMENTAREA_MEDI'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['YEARS_BEGINEXPLUATATION_MEDI'] = new_df['YEARS_BEGINEXPLUATATION_MEDI'].apply(lambda val: 0.95 if val < 0.95 else val)\n",
    "    new_df['YEARS_BUILD_MEDI'] = new_df['YEARS_BUILD_MEDI'].apply(lambda val: 0.3 if val < 0.3 else val)\n",
    "    new_df['COMMONAREA_MEDI'] = new_df['COMMONAREA_MEDI'].apply(lambda val: 0.25 if val > 0.25 else val)\n",
    "    new_df['ELEVATORS_MEDI'] = new_df['ELEVATORS_MEDI'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['ENTRANCES_MEDI'] = new_df['ENTRANCES_MEDI'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['FLOORSMAX_MEDI'] = new_df['FLOORSMAX_MEDI'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['FLOORSMIN_MEDI'] = new_df['FLOORSMIN_MEDI'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['LANDAREA_MEDI'] = new_df['LANDAREA_MEDI'].apply(lambda val: 0.4 if val > 0.4 else val)\n",
    "    new_df['LIVINGAPARTMENTS_MEDI'] = new_df['LIVINGAPARTMENTS_MEDI'].apply(lambda val: 0.6 if val > 0.6 else val)\n",
    "    new_df['LIVINGAREA_MEDI'] = new_df['LIVINGAREA_MEDI'].apply(lambda val: 0.75 if val > 0.75 else val)\n",
    "    new_df['NONLIVINGAPARTMENTS_MEDI'] = new_df['NONLIVINGAPARTMENTS_MEDI'].apply(lambda val: 0.075 if val > 0.075 else val)\n",
    "    new_df['NONLIVINGAREA_MEDI'] = new_df['NONLIVINGAREA_MEDI'].apply(lambda val: 0.2 if val > 0.2 else val)\n",
    "    new_df['FONDKAPREMONT_MODE'] = new_df['FONDKAPREMONT_MODE']\n",
    "    new_df['HOUSETYPE_MODE'] = new_df['HOUSETYPE_MODE']\n",
    "    new_df['TOTALAREA_MODE'] = new_df['TOTALAREA_MODE'].apply(lambda val: 0.60 if val > 0.60 else val)\n",
    "    new_df['WALLSMATERIAL_MODE'] = new_df['WALLSMATERIAL_MODE']\n",
    "    new_df['EMERGENCYSTATE_MODE'] = new_df['EMERGENCYSTATE_MODE']\n",
    "    new_df['OBS_30_CNT_SOCIAL_CIRCLE'] = new_df['OBS_30_CNT_SOCIAL_CIRCLE'].apply(lambda val: 25 if val > 25 else val)\n",
    "    new_df['DEF_30_CNT_SOCIAL_CIRCLE'] = new_df['DEF_30_CNT_SOCIAL_CIRCLE'].apply(lambda val: 5 if val > 5 else val)\n",
    "    new_df['OBS_60_CNT_SOCIAL_CIRCLE'] = new_df['OBS_60_CNT_SOCIAL_CIRCLE'].apply(lambda val: 15 if val > 15 else val)\n",
    "    new_df['DEF_60_CNT_SOCIAL_CIRCLE'] = new_df['DEF_60_CNT_SOCIAL_CIRCLE'].apply(lambda val: 3 if val > 3 else val)\n",
    "    new_df['DAYS_LAST_PHONE_CHANGE'] = new_df['DAYS_LAST_PHONE_CHANGE'].apply(lambda val: -3200 if val < -3200 else val)\n",
    "    new_df['FLAG_DOCUMENT_2'] = new_df['FLAG_DOCUMENT_2']\n",
    "    new_df['FLAG_DOCUMENT_3'] = new_df['FLAG_DOCUMENT_3']\n",
    "    new_df['FLAG_DOCUMENT_4'] = new_df['FLAG_DOCUMENT_4']\n",
    "    new_df['FLAG_DOCUMENT_5'] = new_df['FLAG_DOCUMENT_5']\n",
    "    new_df['FLAG_DOCUMENT_6'] = new_df['FLAG_DOCUMENT_6']\n",
    "    new_df['FLAG_DOCUMENT_7'] = new_df['FLAG_DOCUMENT_7']\n",
    "    new_df['FLAG_DOCUMENT_8'] = new_df['FLAG_DOCUMENT_8']\n",
    "    new_df['FLAG_DOCUMENT_9'] = new_df['FLAG_DOCUMENT_9']\n",
    "    new_df['FLAG_DOCUMENT_10'] = new_df['FLAG_DOCUMENT_10']\n",
    "    new_df['FLAG_DOCUMENT_11'] = new_df['FLAG_DOCUMENT_11']\n",
    "    new_df['FLAG_DOCUMENT_12'] = new_df['FLAG_DOCUMENT_12']\n",
    "    new_df['FLAG_DOCUMENT_13'] = new_df['FLAG_DOCUMENT_13']\n",
    "    new_df['FLAG_DOCUMENT_14'] = new_df['FLAG_DOCUMENT_14']\n",
    "    new_df['FLAG_DOCUMENT_15'] = new_df['FLAG_DOCUMENT_15']\n",
    "    new_df['FLAG_DOCUMENT_16'] = new_df['FLAG_DOCUMENT_16']\n",
    "    new_df['FLAG_DOCUMENT_17'] = new_df['FLAG_DOCUMENT_17']\n",
    "    new_df['FLAG_DOCUMENT_18'] = new_df['FLAG_DOCUMENT_18']\n",
    "    new_df['FLAG_DOCUMENT_19'] = new_df['FLAG_DOCUMENT_19']\n",
    "    new_df['FLAG_DOCUMENT_20'] = new_df['FLAG_DOCUMENT_20']\n",
    "    new_df['FLAG_DOCUMENT_21'] = new_df['FLAG_DOCUMENT_21']\n",
    "    new_df['AMT_REQ_CREDIT_BUREAU_HOUR'] = new_df['AMT_REQ_CREDIT_BUREAU_HOUR'].apply(lambda val: 2.0 if val > 2.0 else val)\n",
    "    new_df['AMT_REQ_CREDIT_BUREAU_DAY'] = new_df['AMT_REQ_CREDIT_BUREAU_DAY'].apply(lambda val: 4.0 if val > 4.0 else val)\n",
    "    new_df['AMT_REQ_CREDIT_BUREAU_WEEK'] = new_df['AMT_REQ_CREDIT_BUREAU_WEEK'].apply(lambda val: 3.0 if val > 3.0 else val)\n",
    "    new_df['AMT_REQ_CREDIT_BUREAU_MON'] = new_df['AMT_REQ_CREDIT_BUREAU_MON'].apply(lambda val: 17.0 if val > 17.0 else val)\n",
    "    new_df['AMT_REQ_CREDIT_BUREAU_QRT'] = new_df['AMT_REQ_CREDIT_BUREAU_QRT'].apply(lambda val: 6.0 if val > 6.0 else val)\n",
    "    new_df['AMT_REQ_CREDIT_BUREAU_YEAR'] = new_df['AMT_REQ_CREDIT_BUREAU_YEAR'].apply(lambda val: 14.0 if val > 14.0 else val)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_credit_application = clean_data(df=raw_train_credit_application)\n",
    "test_credit_application = clean_data(df=raw_test_credit_application, train_df=raw_train_credit_application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_feature(df):\n",
    "    columns = ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', \n",
    "               'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', \n",
    "               'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', \n",
    "               'OCCUPATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE', \n",
    "               'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', \n",
    "               'EMERGENCYSTATE_MODE']\n",
    "    for col in columns:\n",
    "        if col in df:\n",
    "            df = pd.get_dummies(df, columns=[col])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_credit_application = engineer_feature(train_credit_application)\n",
    "test_credit_application = engineer_feature(test_credit_application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_credit_application = train_credit_application.reindex(np.random.permutation(train_credit_application.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution if the randomization is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = train_credit_application['SK_ID_CURR'].count()\n",
    "training_count = math.ceil(total_count * 0.75)\n",
    "validation_count = math.floor(total_count * 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_credit_application = train_credit_application.head(training_count)\n",
    "validation_credit_application = train_credit_application.tail(validation_count)\n",
    "testing_credit_application = test_credit_application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ids_data_targets(df, dataframe=False):\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    new_df_ids = new_df['SK_ID_CURR']\n",
    "    if 'TARGET' in new_df:\n",
    "        new_df_targets = new_df['TARGET']\n",
    "        new_df_data = new_df.drop(columns=['SK_ID_CURR', 'TARGET'])\n",
    "    else:\n",
    "        new_df_targets = None\n",
    "        new_df_data = new_df.drop(columns=['SK_ID_CURR'])\n",
    "    \n",
    "    \n",
    "    if not dataframe:\n",
    "        new_df_ids = new_df_ids.values\n",
    "        new_df_data = new_df_data.values\n",
    "        if new_df_targets is not None:\n",
    "            new_df_targets = new_df_targets.values\n",
    "    \n",
    "    return new_df_ids, new_df_data, new_df_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ids, training_data, training_targets = split_ids_data_targets(training_credit_application)\n",
    "validation_ids, validation_data, validation_targets = split_ids_data_targets(validation_credit_application)\n",
    "testing_ids, testing_data, testing_targets = split_ids_data_targets(testing_credit_application)\n",
    "\n",
    "training_targets_onehot = (preprocessing.OneHotEncoder().fit_transform(training_targets.reshape(-1, 1))).toarray()\n",
    "validation_targets_onehot = (preprocessing.OneHotEncoder().fit_transform(validation_targets.reshape(-1, 1))).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model, ensemble, svm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sgd_classifier =  linear_model.SGDClassifier()\n",
    "sgd_classifier.fit(training_data, training_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "svm_classifier = svm.SVC(kernel='rbf')\n",
    "svm_classifier.fit(training_data, training_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_metric(y_true, y_pred):\n",
    "    return tf.Variable(metrics.roc_auc_score(y_true, y_pred), name='auc_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_activation = 'sigmoid'\n",
    "default_last_activation = 'sigmoid'\n",
    "default_batch_size = 1000\n",
    "default_epochs = 10\n",
    "\n",
    "default_dnn_classifier_settings = [\n",
    "    # Adam\n",
    "    {'optimizer': optimizers.Adam(), 'batch_size': default_batch_size, 'epochs': default_epochs, \n",
    "     'activation': default_activation, 'last_activation': default_last_activation,\n",
    "     'acc': None, 'val_acc': None, 'history': None, 'auc': None, 'val_auc': None},\n",
    "    \n",
    "    # SGD\n",
    "#     {'optimizer': optimizers.SGD(momentum=0.1), 'batch_size': default_batch_size, 'epochs': default_epochs * 10,\n",
    "#      'activation': default_activation, 'last_activation': default_last_activation,\n",
    "#      'acc': None, 'val_acc': None, 'history': None, 'auc': None, 'val_auc': None},\n",
    "    \n",
    "    # Adagrad\n",
    "    {'optimizer': optimizers.Adagrad(), 'batch_size': default_batch_size, 'epochs': default_epochs, \n",
    "     'activation': default_activation, 'last_activation': default_last_activation,\n",
    "     'acc': None, 'val_acc': None, 'history': None, 'auc': None, 'val_auc': None},\n",
    "    \n",
    "    # RMSprop\n",
    "    {'optimizer': optimizers.RMSprop(), 'batch_size': default_batch_size, 'epochs': default_epochs, \n",
    "     'activation': default_activation, 'last_activation': default_last_activation,\n",
    "     'acc': None, 'val_acc': None, 'history': None, 'auc': None, 'val_auc': None},\n",
    "    \n",
    "    # Adamax\n",
    "    {'optimizer': optimizers.Adamax(), 'batch_size': default_batch_size, 'epochs': default_epochs, \n",
    "     'activation': default_activation, 'last_activation': default_last_activation,\n",
    "     'acc': None, 'val_acc': None, 'history': None, 'auc': None, 'val_auc': None},\n",
    "    \n",
    "    # Nadam\n",
    "    {'optimizer': optimizers.Nadam(), 'batch_size': default_batch_size, 'epochs': default_epochs, \n",
    "     'activation': default_activation, 'last_activation': default_last_activation,\n",
    "     'acc': None, 'val_acc': None, 'history': None, 'auc': None, 'val_auc': None},\n",
    "]\n",
    "\n",
    "dnn_classifier_settings = []\n",
    "dnn_classifier_settings.extend(default_dnn_classifier_settings)\n",
    "\n",
    "new_settings = [\n",
    "]\n",
    "\n",
    "if len(new_settings) > 0:\n",
    "    dnn_classifier_settings.extend(new_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index, dnn_classifier_setting in enumerate(dnn_classifier_settings):\n",
    "    if dnn_classifier_setting['acc'] is None and dnn_classifier_setting['val_acc'] is None and dnn_classifier_setting['history'] is None:\n",
    "        optimizer = dnn_classifier_setting['optimizer']\n",
    "        batch_size = dnn_classifier_setting['batch_size']\n",
    "        epochs = dnn_classifier_setting['epochs']\n",
    "        activation = dnn_classifier_setting['activation']\n",
    "        last_activation = dnn_classifier_setting['last_activation']\n",
    "\n",
    "        dnn_classifier = Sequential()\n",
    "        input_shape = (training_data.shape[1], )\n",
    "        dnn_classifier.add(Dense(128, activation=activation, input_shape=input_shape))\n",
    "        dnn_classifier.add(Dropout(rate=0.35))\n",
    "        dnn_classifier.add(Dense(128, activation=activation))\n",
    "        dnn_classifier.add(Dropout(rate=0.35))\n",
    "        dnn_classifier.add(Dense(64, activation=activation))\n",
    "        dnn_classifier.add(Dropout(rate=0.25))\n",
    "        dnn_classifier.add(Dense(2, activation=last_activation))\n",
    "        dnn_classifier.compile(loss='binary_crossentropy', \n",
    "                               optimizer=optimizer,\n",
    "                               metrics=['acc'])\n",
    "        history = dnn_classifier.fit(training_data, training_targets_onehot,\n",
    "                          epochs=epochs, batch_size=batch_size, verbose=0, shuffle=True,\n",
    "                          validation_data=(validation_data, validation_targets_onehot))\n",
    "\n",
    "        dnn_classifier_setting['history'] = history\n",
    "        \n",
    "        training_predictions = dnn_classifier.predict(training_data)\n",
    "        training_predictions = pd.DataFrame(training_predictions).apply(lambda val: 1.0 if val[1] > 0.50 else 0.0, axis=1)\n",
    "        \n",
    "        validation_predictions = dnn_classifier.predict(validation_data)\n",
    "        validation_predictions = pd.DataFrame(validation_predictions).apply(lambda val: 1.0 if val[1] > 0.50 else 0.0, axis=1)\n",
    "        \n",
    "        dnn_classifier_setting['acc'] = metrics.accuracy_score(training_targets, training_predictions)\n",
    "        dnn_classifier_setting['val_acc'] = metrics.accuracy_score(validation_targets, validation_predictions)\n",
    "        dnn_classifier_setting['auc'] = metrics.roc_auc_score(training_targets, training_predictions)\n",
    "        dnn_classifier_setting['val_auc'] = metrics.roc_auc_score(validation_targets, validation_predictions)\n",
    "\n",
    "    print('%2d: Optimizer: %10s; LR: %.5f; bs: %3d; epochs: %4d; acc: %.4f; val_acc: %.4f; auc: %.2f; val_auc: %.2f' % (index, \n",
    "                                                                                              type(dnn_classifier_setting['optimizer']).__name__, \n",
    "                                                                                              dnn_classifier_setting['optimizer'].get_config()['lr'], \n",
    "                                                                                              dnn_classifier_setting['batch_size'], \n",
    "                                                                                              dnn_classifier_setting['epochs'],\n",
    "                                                                                              dnn_classifier_setting['acc'], \n",
    "                                                                                              dnn_classifier_setting['val_acc'],\n",
    "                                                                                              dnn_classifier_setting['auc'],\n",
    "                                                                                              dnn_classifier_setting['val_auc']))\n",
    "\n",
    "IPython.display.Audio('http://www.pacdv.com/sounds/interface_sound_effects/sound94.wav', autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = dnn_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions = classifier.predict(training_data)\n",
    "print(training_predictions)\n",
    "training_predictions = pd.DataFrame(training_predictions).apply(lambda val: 1.0 if val[1] > 0.1 else 0.0, axis=1)\n",
    "validation_predictions = classifier.predict(validation_data)\n",
    "validation_predictions = pd.DataFrame(validation_predictions).apply(lambda val: 1.0 if val[1] > 0.1 else 0.0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(training_targets).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(training_targets, training_predictions))\n",
    "print(metrics.roc_auc_score(training_targets, training_predictions))\n",
    "print(metrics.accuracy_score(validation_targets, validation_predictions))\n",
    "print(metrics.roc_auc_score(validation_targets, validation_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare submission.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
